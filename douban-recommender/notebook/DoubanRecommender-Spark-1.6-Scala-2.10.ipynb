{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/* def main(args: Array[String]): Unit = {\n",
    "   val sc = new SparkContext(new SparkConf().setAppName(\"DoubanRecommender\"))\n",
    "    val DataHome = if (args.length > 0) args(0) else \"file:///Users/hadoop/eda/data/douban/\"\n",
    "    val rawUserMoviesData = sc.textFile(DataHome + \"user_movies.csv\")\n",
    "} */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:19: error: not found: value args\n",
       "       val DataHome = if (args.length > 0) args(0) else \"file:///Users/hadoop/eda/data/douban/\"\n",
       "                          ^\n",
       "<console>:19: error: not found: value args\n",
       "       val DataHome = if (args.length > 0) args(0) else \"file:///Users/hadoop/eda/data/douban/\"\n",
       "                                           ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val sc = new SparkContext(new SparkConf().setAppName(\"DoubanRecommender\"))\n",
    "//val DataHome = if (args.length > 0) args(0) else \"file:///Users/hadoop/eda/data/douban/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scala.collection.Map\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.mllib.recommendation._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val DataHome = \"file:///Users/hadoop/eda/data/douban/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///Users/hadoop/eda/data/douban/"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataHome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rawUserMoviesData = sc.textFile(DataHome + \"user_movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///Users/hadoop/eda/data/douban/user_movies.csv MapPartitionsRDD[2] at textFile at <console>:22"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawUserMoviesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adamwzw,20645098,4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawUserMoviesData.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///Users/hadoop/eda/data/douban/user_movies.csv MapPartitionsRDD[2] at textFile at <console>:22"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawUserMoviesData.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 223239, mean: 111620.188663, stdev: 64445.607152, max: 223966.000000, min: 0.000000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawUserMoviesData.map(_.split(',')(0).trim).distinct().zipWithUniqueId().map(_._2.toDouble).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 165, mean: 20734733.139394, stdev: 8241677.225813, max: 26599083.000000, min: 1866473.000000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawUserMoviesData.map(_.split(',')(1).trim.toDouble).distinct().stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 165, mean: 20734733.139394, stdev: 8241677.225813, max: 26599083.000000, min: 1866473.000000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawUserMoviesData.map(_.split(',')(1).trim.toDouble).distinct().stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rawHotMoviesData = sc.textFile(DataHome + \"hot_movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///Users/hadoop/eda/data/douban/hot_movies.csv MapPartitionsRDD[4] at textFile at <console>:22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawHotMoviesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20645098,8.2,小王子"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawHotMoviesData.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildMovies(rawHotMoviesData: RDD[String]): Map[Int, String] =\n",
    "rawHotMoviesData.flatMap {\n",
    "line => val tokens = line.split(',')\n",
    "if (tokens(0).isEmpty){\n",
    "None\n",
    "} else {\n",
    "Some((tokens(0).toInt, tokens(2)))\n",
    "}\n",
    "}.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preparation(rawUserMoviesData: RDD[String],\n",
    "                rawHotMoviesData: RDD[String]\n",
    ") = {\n",
    "val userIDStats = rawUserMoviesData.map(_.split(',')(0).trim).distinct().zipWithUniqueId().map(_._2.toDouble).stats()\n",
    "val itemIDStats = rawUserMoviesData.map(_.split(',')(1).trim.toDouble).distinct().stats()\n",
    "println(userIDStats)\n",
    "println(itemIDStats)\n",
    "val moviesAndName = buildMovies(rawHotMoviesData)\n",
    "val (movieID, movieName) = moviesAndName.head\n",
    "println(movieID + \" -> \" + movieName)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count: 223239, mean: 111620.188663, stdev: 64445.607152, max: 223966.000000, min: 0.000000)\n",
      "(count: 165, mean: 20734733.139394, stdev: 8241677.225813, max: 26599083.000000, min: 1866473.000000)\n",
      "6866928 -> 进击的巨人真人版：前篇\n"
     ]
    }
   ],
   "source": [
    "preparation(rawUserMoviesData, rawHotMoviesData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class MovieRating(userID: String, movieID: Int, rating: Double) extends\n",
    "     scala.Serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildRatings(rawUserMoviesData: RDD[String]): RDD[MovieRating] = {\n",
    "    rawUserMoviesData.map {\n",
    "        line =>\n",
    "                val Array(userID, moviesID, countStr) = line.split(',').map(_.trim)\n",
    "                var count = countStr.toInt\n",
    "                count = if (count == -1) 3 else count\n",
    "                MovieRating(userID, moviesID.toInt, count)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Check result of recommendation to one user\n",
    "def checkRecommenderResult(userID: Int, rawUserMoviesData: RDD[String],\n",
    "                          bMoviesAndName: Broadcast[Map[Int, String]],\n",
    "                          reverseUserIDMapping: RDD[(Long, String)],\n",
    "                          model: MatrixFactorizationModel): Unit ={\n",
    "    val userName = reverseUserIDMapping.lookup(userID).head\n",
    "    val recommendations = model.recommendProducts(userID, 5)\n",
    "    // The recommendation sets to the user 5.\n",
    "    val recommendedMovieIDs = recommendations.map(_.product).toSet\n",
    "    \n",
    "    // Get the movies set which the user 5 has played.\n",
    "    val rawMoviesForUser = rawUserMoviesData.map(_.split(',')).filter{\n",
    "    case Array(user, _, _) => user.trim == userName}\n",
    "    \n",
    "    val existingUserMovieIDs = rawMoviesForUser.map {\n",
    "    case Array(_, movieID, _) => movieID.toInt\n",
    "    }.collect().toSet\n",
    "    \n",
    "    println(\"Recommend to user \" + userName + \" the movie name:\")\n",
    "    //Recommendating movies name\n",
    "    bMoviesAndName.value.filter {\n",
    "    case (id, name) => recommendedMovieIDs.contains(id)\n",
    "    }.values.foreach(println)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpersist(model: MatrixFactorizationModel): Unit = {\n",
    "    // At the moment, it's necessary to manually unpersist the RDDs inside the model\n",
    "    // when done with it in order to make sure they are promptly uncached\n",
    "    model.userFeatures.unpersist()\n",
    "    model.productFeatures.unpersist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(sc: SparkContext, \n",
    "          rawUserMoviesData: RDD[String],\n",
    "          rawHotMoviesData: RDD[String]): Unit = {\n",
    "    val moviesAndName = buildMovies(rawHotMoviesData)\n",
    "    val bMoviesAndName = sc.broadcast(moviesAndName)\n",
    "    val data = buildRatings(rawUserMoviesData)\n",
    "    val userIdToInt: RDD[(String, Long)] = \n",
    "        data.map(_.userID).distinct().zipWithUniqueId()\n",
    "    val reverseUserIDMapping: RDD[(Long, String)] =\n",
    "        userIdToInt map {case (l, r) => (r, l)}\n",
    "    val userIDMap: Map[String, Int] = userIdToInt.collectAsMap().map{\n",
    "        case (n, l) => (n, l.toInt)\n",
    "        }\n",
    "    val bUserIDMap = sc.broadcast(userIDMap)\n",
    "    val ratings: RDD[Rating] = data.map {\n",
    "        r => Rating(bUserIDMap.value.get(r.userID).get, r.movieID, r.rating)\n",
    "        }.cache()\n",
    "        \n",
    "    //Build model with Collabrative Filtering\n",
    "    val model = ALS.train(ratings, 50, 10, 0.0001)\n",
    "    ratings.unpersist()\n",
    "    println(\"The First User Feature is Output: \")\n",
    "    println(model.userFeatures.mapValues(_.mkString(\",\")).first())\n",
    "    for (userID <- Array(100, 1001, 10001, 100001, 110000)){\n",
    "        checkRecommenderResult(userID, rawUserMoviesData, \n",
    "        bMoviesAndName, reverseUserIDMapping, model)\n",
    "    }\n",
    "    \n",
    "    unpersist(model)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First User Feature is Output: \n",
      "(0,-0.2707848846912384,-0.8239490985870361,-0.2288040816783905,-1.3201241493225098,-0.25197556614875793,-0.7580539584159851,0.39071187376976013,1.3047521114349365,-0.5541315674781799,-1.0833860635757446,0.21554742753505707,0.3992951810359955,-1.3039758205413818,-0.8273643851280212,0.2698359489440918,0.9498853087425232,0.7356423735618591,0.16813980042934418,0.15691663324832916,1.1658538579940796,-0.9022631645202637,-0.7636549472808838,-0.5596442818641663,0.7390807271003723,0.9053812026977539,0.6829679012298584,0.4719841182231903,0.24569830298423767,-0.6468056440353394,2.3752706050872803,-0.631567656993866,-0.8765032291412354,-1.2385278940200806,0.808280348777771,-1.1208305358886719,-2.5614330768585205,-0.8047855496406555,-1.3208379745483398,0.5838742852210999,-0.3246144950389862,-0.3470260798931122,0.343004435300827,-0.41452157497406006,-0.7579929828643799,0.4793323874473572,-0.6534832715988159,0.17293120920658112,1.6464449167251587,-0.7856852412223816,-1.3987237215042114)\n",
      "Recommend to user yimiao the movie name:\n",
      "传奇\n",
      "极道大战争\n",
      "妈妈的朋友\n",
      "机动战士高达 THE ORIGIN I 青瞳的卡斯巴尔\n",
      "如此美好\n",
      "Recommend to user nianyis the movie name:\n",
      "白河夜船\n",
      "王牌特工：特工学院\n",
      "抢劫\n",
      "出租车\n",
      "机动战士高达 THE ORIGIN I 青瞳的卡斯巴尔\n",
      "Recommend to user infi_l the movie name:\n",
      "白河夜船\n",
      "流浪者年代记\n",
      "王牌特工：特工学院\n",
      "极道大战争\n",
      "机动战士高达 THE ORIGIN I 青瞳的卡斯巴尔\n",
      "Recommend to user 35049396 the movie name:\n",
      "传奇\n",
      "白河夜船\n",
      "抢劫\n",
      "刺客聂隐娘\n",
      "机动战士高达 THE ORIGIN I 青瞳的卡斯巴尔\n",
      "Recommend to user 77151678 the movie name:\n",
      "白河夜船\n",
      "橘色\n",
      "抢劫\n",
      "有客到\n",
      "斯坦福监狱实验\n"
     ]
    }
   ],
   "source": [
    "model(sc, rawUserMoviesData, rawHotMoviesData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recommend(sc: SparkContext,\n",
    "                rawUserMoviesData: RDD[String],\n",
    "                rawHotMoviesData: RDD[String],\n",
    "                base:String): Unit = {\n",
    "    val moviesAndName = buildMovies(rawHotMoviesData)\n",
    "    val bMoviesAndName = sc.broadcast(moviesAndName)\n",
    "\n",
    "    val data = buildRatings(rawUserMoviesData)\n",
    "\n",
    "    val userIdToInt: RDD[(String, Long)] =\n",
    "      data.map(_.userID).distinct().zipWithUniqueId()\n",
    "    val reverseUserIDMapping: RDD[(Long, String)] =\n",
    "      userIdToInt map { case (l, r) => (r, l) }\n",
    "\n",
    "    val userIDMap: Map[String, Int] =\n",
    "      userIdToInt.collectAsMap().map { case (n, l) => (n, l.toInt) }\n",
    "\n",
    "    val bUserIDMap = sc.broadcast(userIDMap)\n",
    "    val bReverseUserIDMap = sc.broadcast(reverseUserIDMapping.collectAsMap())\n",
    "\n",
    "    val ratings: RDD[Rating] = data.map { r =>\n",
    "      Rating(bUserIDMap.value.get(r.userID).get, r.movieID, r.rating)\n",
    "    }.cache()\n",
    "    //使用协同过滤算法建模\n",
    "    //val model = ALS.trainImplicit(ratings, 10, 10, 0.01, 1.0)\n",
    "    val model = ALS.train(ratings, 50, 10, 0.0001)\n",
    "    ratings.unpersist()\n",
    "\n",
    "    model.save(sc, base+\"model\")\n",
    "    //val sameModel = MatrixFactorizationModel.load(sc, base + \"model\")\n",
    "\n",
    "    val allRecommendations = model.recommendProductsForUsers(5) map {\n",
    "      case (userID, recommendations) => {\n",
    "        var recommendationStr = \"\"\n",
    "        for (r <- recommendations) {\n",
    "          recommendationStr += r.product + \":\" + bMoviesAndName.value.getOrElse(r.product, \"\") + \",\"\n",
    "        }\n",
    "        if (recommendationStr.endsWith(\",\"))\n",
    "          recommendationStr = recommendationStr.substring(0,recommendationStr.length-1)\n",
    "\n",
    "        (bReverseUserIDMap.value.get(userID).get,recommendationStr)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    //allRecommendations.saveAsTextFile(base + \"result.csv\")\n",
    "    allRecommendations.coalesce(1).sortByKey().saveAsTextFile(base + \"result.csv\")\n",
    "\n",
    "    unpersist(model)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.RuntimeException\n",
       "Message: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n",
       "StackTrace: org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\n",
       "org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204)\n",
       "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
       "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
       "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
       "org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:249)\n",
       "org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:327)\n",
       "org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:237)\n",
       "org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:457)\n",
       "org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:457)\n",
       "org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:456)\n",
       "org.apache.spark.sql.hive.HiveContext$$anon$3.<init>(HiveContext.scala:473)\n",
       "org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:473)\n",
       "org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:472)\n",
       "org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:34)\n",
       "org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)\n",
       "org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:52)\n",
       "org.apache.spark.sql.SQLContext.createDataFrame(SQLContext.scala:417)\n",
       "org.apache.spark.sql.SQLImplicits.rddToDataFrameHolder(SQLImplicits.scala:155)\n",
       "org.apache.spark.mllib.recommendation.MatrixFactorizationModel$SaveLoadV1_0$.save(MatrixFactorizationModel.scala:361)\n",
       "org.apache.spark.mllib.recommendation.MatrixFactorizationModel.save(MatrixFactorizationModel.scala:205)\n",
       "$line222.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.recommend(<console>:84)\n",
       "$line227.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:59)\n",
       "$line227.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:64)\n",
       "$line227.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:66)\n",
       "$line227.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:68)\n",
       "$line227.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:70)\n",
       "$line227.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:72)\n",
       "$line227.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:74)\n",
       "$line227.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:76)\n",
       "$line227.$read$$iwC$$iwC$$iwC.<init>(<console>:78)\n",
       "$line227.$read$$iwC$$iwC.<init>(<console>:80)\n",
       "$line227.$read$$iwC.<init>(<console>:82)\n",
       "$line227.$read.<init>(<console>:84)\n",
       "$line227.$read$.<init>(<console>:88)\n",
       "$line227.$read$.<clinit>(<console>)\n",
       "$line227.$eval$.<init>(<console>:7)\n",
       "$line227.$eval$.<clinit>(<console>)\n",
       "$line227.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend(sc, rawUserMoviesData, rawHotMoviesData, DataHome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
